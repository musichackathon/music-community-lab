---
title: "Immersive Music Hackathon"
description: "The Monthly Music Hackathon series invites anyone and everyone to come collaborate to celebrate and create immersive music at our event in December."
date: 2017-12-09 11:30am
address: "45 W 18th St, New York, NY 10011"
rsvp: "https://www.eventbrite.com/e/immersive-music-hackathon-tickets-39600372781"
image: "2017-12-09.jpg"
---
# Schedule
- 11:30 Doors open
- 12:00 Opening talks
- 1:00 Hacking begins; Brainstorming and collaborator-search session
- 2:00 Workshops begin
- 7:00 Sign up to present
- 8:00 Presentations begin

# Speakers and Workshops
[**Ken Perlin**](http://mrl.nyu.edu/~perlin/) is a professor in the Department of Computer Science at New York University, directs the Future Reality Lab, and is a participating faculty member at NYU MAGNET. His research interests include future reality, graphics and animation, user interfaces and education.

*Talk* The use of music and audio in supporting immersive live VR theater: We recently presented “Holojam in Wonderland”, a live theatrical performance in which everyone – both actors and audience – were immersed in a virtual world via collocated untethered VR. I will discuss how we used a combination of computer graphics and music/audio to support the magical realist elements of the piece, including transitions between interior and exterior scenes, and large changes in apparent scale of both characters and audience.

**Nicole Stein** is passionate about mixing technology and education – both in building educational technologies and spreading her love of technology through teaching. Nicole works on Google Expeditions - an educational app to bring virtual reality “field trips” into K-12 classrooms using Google Cardboard. She also teaches front-end web development at the New York Code + Design Academy.

*Workshop* Introduction to building VR with A-Frame: Use WebVR with HTML and Entity-Component to build a VR experience that works on Vive, Rift, Daydream, GearVR, or desktop.

**Brennan McTernan** has been a technologist for the past 35 years specializing in a variety of computer graphics technologies. As GM of In It VR, Brennan has concentrated on using VR and AR technologies to help brands fight the flight from brick and mortar. Brennan has diverse experience in the field of computer graphics. He has developed photo processing tools, stereo visualization systems, custom rendering software, and he has developed proprietary motion capture and facial capture technologies. Brennan’s VR experience started when he incorporated VR into his motion capture system. He was a key developer for VR theme park ride “Ride-the-Comix” at Disney World and he developed a full scale VR Nascar Racer Simulator. He was also involved with a major R&D project involving a home VR system for a major toy manufacturer.

*Workshop* How spatial audio works, the kinds of microphones and how they work, and how our ears tell our eyes where to look: This workshop includes example of sounds, Unity app demonstration + sound and activity. Attendees should install (free) Unity 2017.2 + laptop + headphones.

**Pran Bandi** is an award-winning composer and sound designer from Peterborough, UK. His virtual reality work includes Google Earth VR, Zero Days VR, and other pieces for the Oculus Rift, Google Cardboard, and Daydream. He has also worked on installations at the Panorama festival in NYC, and done augmented reality work for the Google Pixel.

*Workshop* Introduction to immersive audio: This workshop introduces techniques for approaching spacial audio, including ambisonics, plugins, and DAW workflow.

[**Charles Berman**](http://bermondo.com) is a multimedia artist, guitarist, composer, and technologist. His multimedia performance vehicle, Bermondo, is an immersive experience that fuses the impressions of sound, sight and sense of place. Mixing these ingredients in equal proportions, pieces are performed with live electric guitar, computer, synthesized reactive video, and infrared motion control. As a guitarist in Boston, New York and New Orleans, Charles has performed with artists including Dr. John, Bobby Rush, James Andrews, Rockin’ Doopsie Jr, the Cadillacs, and Hey Dave. His CD “Poinciana Revisited” was released in 2004 and is available online on Spotify, Apple Music, etc. As a technologist, Charles has designed and coded realtime, distributed, object-oriented Internet systems for the engineering and financial industries.

*Talk* I’ll open with a performance piece for approximately 3-4 minutes. I’ll follow that with a description of how it works from a high level, and then drill down into each of the components: the use of Kinect, Ableton, Max/MSP. I can show some code examples of Max/MSP and broadly how i’m using it. (There are usually questions from the audience, which i’ll suggest we handle after the session). I’ll close with a short piece for another 3-4 minutes.

[**Dana Abrassart**](http://www.danaabrassart.com) is a Brooklyn-based human for whom both science and intuition serve as inspirations. Her recent work has focused on embodied cognition, movement, and biodesign. Dana holds an MSc in Neurobiology and is a recent grad of NYU’s Interactive Telecommuncations Program.

*Talk* Mind, Body, & Sound - Emerging technologies, like artificial intelligence, aspire to mimic human intelligence as defined simply by the mind. But how might technology be used to augment our physical awareness and heighten the connection between mind and body?

[**Brad Garton**](http://sites.music.columbia.edu/brad/) received his PhD. from Princeton University in music composition. He currently serves on the faculty of Columbia University, where he is Director of the Computer Music Center.

**Ethan Edwards** is a programmer and artist currently based in New York City. His work explores traditional aesthetic themes through interactive and generative structures.

**Onur Yıldırım** is a Turkish composer currently based in New York, where he pursues doctoral studies in composition at Columbia University. His music is informed by computational phonetics, comparative linguistics and machine learning.

*Talk* Integrated Music Generation and Interactive Synthesis/DSP in Unity using RTcmix: An overview of the RTcmix sound synthesis and digital signal processing music language within Unity, featuring algorithmic music generation capabilities and advanced DSP capabilities. These are all accessible within the Unity game development environment. Of special note is the ability to attach individual processes to specific game objects. RTcmix works using the extant Unity audio system.

*Workshop* Using RTcmix in the Unity Game Development Environment (intermediate level): We will walk through the initialization of RTcmix within Unity, and then show how to accomplish several audio tasks: 1. start and modify an algorithmic/generative music process that ‘tracks’ parameters that are set within Unity; 2. build an interactive synthesis object that allows Unity to control sound parameters dynamically; and 3. Start several distinct music processes that respond to actions upon particular game objects (time permitting). Some familiarity with the Unity scripting language (C#) is assumed.
